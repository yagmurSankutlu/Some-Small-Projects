{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This assessment aims to make you familiar with word embeddings and fine-tuning them for a specific downstream task. This assignment will be in two parts: **Part1: Fasttext** and **Part2: POS Tagging**. The due date of the exercise is **16th of April, 11.59pm**. You are going to submit your work via Blackboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Install PyTorch (If you are working locally)\n",
    "\n",
    "1. Have the latest version of Anaconda installed on your machine.\n",
    "2. Create a new conda environment starting from Python 3.7. In this setup example, we'll call it `torch_env`.\n",
    "3. Run the command: `conda activate torch_env`\n",
    "4. Run the command: `pip3 install torch==1.13.1`\n",
    "\n",
    "## or just work on this notebook on google colab.\n",
    "https://colab.research.google.com/\n",
    "\n",
    "For this assignment, you don't need a GPU to train the models. However, if you want to use a GPU, you can follow the steps below to create a GPU backed environment. https://www.youtube.com/watch?v=TI9mTiTKoUc&ab_channel=SinaTofighi This video shows how you can open a Colab, request and allocate a GPU for your work. You can follow the steps to create your GPU backed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP542/442 - Assignment 1 - Part 1\n",
    "\n",
    "The assignment consists of the following parts:\n",
    "* **Part I**: Preparation\n",
    "  * Installing the required packages\n",
    "  * Downloading data\n",
    "  * Preprocessing\n",
    "* **Part II**: Model Training\n",
    "  * Training word embeddings on the Turkish dataset\n",
    "  * Training with Contuinuous Bag of Words approach (Optional)\n",
    "  * Trianing with Skipgram approach\n",
    "* **Part III**: Observations\n",
    "  * Make observations for get_nearest_neighbors and get_analogies method\n",
    "  * Compare CBOW with Skipgram (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inline question 1: Describe n-gram, BPE vs wordpiece/unigram tokenization methods with one or two sentences. Please compare their advantages and disadvantages over each other.\n",
    "\n",
    "<font color='red'>Your answer:</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing fastText\n",
    "\n",
    "You may follow the instruction from the documentation:\n",
    "* https://fasttext.cc/docs/en/support.html\n",
    "* https://fasttext.cc/docs/en/unsupervised-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/fastText.git\n",
    "%cd fastText\n",
    "!pip install .\n",
    "!python setup.py install\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if installation was successful\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following data for training the embeddings: trwiki-20230401-pages-articles-multistream.xml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download preprocessing script**\n",
    "\n",
    "A raw Wikipedia dump contains a lot of HTML / XML data, for preprocessing it, you may use the script from: https://github.com/hghodrati/wikifil.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hghodrati/wikifil.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess xml and save to new file\n",
    "!perl wikifil/wikifil.pl dataset/trwiki-20230401-pages-articles-multistream.xml > dataset/data_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "!head -c 80 dataset/data_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with fastText\n",
    "\n",
    "You may find the documentation for training word respresentations here: \n",
    "* https://fasttext.cc/docs/en/unsupervised-tutorial.html\n",
    "* https://fasttext.cc/docs/en/python-module.html#train_unsupervised-parameters\n",
    "\n",
    "You may use the embedding dimension as 100, which is default by fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to train the word embedding using two approaches:\n",
    "* Continuous Bag of words\n",
    "* Skipgram\n",
    "After training, save the models in their respective paths\n",
    "\n",
    "You may refer to the tutorial/ documentation for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir results\n",
    "CBOW_EMBED = \"results/embed_cbow.bin\"\n",
    "SKIPGRAM_EMBED = \"results/embed_skipgram.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_model_cbow = None\n",
    "# embed_model_cbow.save_model(CBOW_EMBED)\n",
    "# embed_model_cbow = fasttext.load_model(CBOW_EMBED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_model_skipgram = None\n",
    "# embed_model_skipgram.save_model(SKIPGRAM_EMBED)\n",
    "# embed_model_skipgram = fasttext.load_model(SKIPGRAM_EMBED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_model_cbow.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_model_cbow['kral']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_cbow.get_nearest_neighbors(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_model_skipgram['kral']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_skipgram.get_nearest_neighbors(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline Question 2: Find an example of an analogy that holds, using the `get_analogies` function. Explain the analogy and also how the analogies are calculated.\n",
    "\n",
    "<font color='red'>Your answer:</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_skipgram.get_analogies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP542/442 - Assignment 1 - Part 2\n",
    "\n",
    "In this assignment, we are implementing an RNN-based POS (Part-of-Speech) tagger using BiLSTM (bidirectional Long Short-Term Memory) networks. \n",
    "\n",
    "The assignment consists of the following parts:\n",
    "* **Part I**: Preparation\n",
    "  * Installing the required packages\n",
    "  * Data loading and preprocessing\n",
    "  * Creating the datasets and dataloaders\n",
    "* **Part II**: Model Implementation and Training\n",
    "  * Implementing the BiLSTMPOS Tagger model\n",
    "  * Defining training and evaluation functions\n",
    "  * Running the training loop and observing the loss and accuracy\n",
    "  * Plotting the training metrics such as loss and accuracy\n",
    "  * Saving the model\n",
    "* **Part III**: Initializating BiLSTM with fastText Embeddings\n",
    "  * Loading the fastText model\n",
    "  * Initializating with fastText and BiLSTM models\n",
    "  * Training the model\n",
    "  * Evaluating the model\n",
    "\n",
    "Throughout the assignment, you will work with a POS dataset to train and test the model to recognize different POS tags for the given sentences. You also have the option to use this notebook on Google Colab that allows you to allocate a GPU for faster training.\n",
    "\n",
    "For more details about the POS tags, check the following link: https://universaldependencies.org/tr/pos/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the Part-of-the-Speech (POS) dataset. Make sure you have downloaded the dataset using the provided script. Check the assignment handout for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the seeds for reproducibility\n",
    "SEED = 542\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses a file in the Universal Dependencies (UD) annotation style and returns a list of all the sentences in the file.\n",
    "    Note: The data files you need in this part of the assignemnt are stored under the dataset/ directory. You can open the\n",
    "    files to have a better understanding of the format. If you want to learn more about the POS tags, you can visit the\n",
    "    Universal Dependencies website: https://universaldependencies.org/tr/pos/index.html\n",
    "\n",
    "    The output should be a list of tuples, where each tuple represents a sentence and contains (word, POS tag) pairs for each\n",
    "    word in the sentence. For example, the following sentence:\n",
    "\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "    should be represented as:\n",
    "    [(\"The\", \"DET\"), (\"quick\", \"ADJ\"), (\"brown\", \"ADJ\"), (\"fox\", \"NOUN\"), (\"jumps\", \"VERB\"), (\"over\", \"ADP\"), (\"the\", \"DET\"), (\"lazy\", \"ADJ\"), (\"dog\", \"NOUN\"), (\".\", \"PUNCT\")]\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): The path to the file to be parsed.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tuples, where each tuple represents a sentence and contains (word, POS tag) pairs for each word in the sentence.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE*****\n",
    "    \n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data: List[List[Tuple[str, str]]]) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of words and part-of-speech (POS) tags based on the input data. Don't forget to add special tokens (e.g. <PAD>, <UNK>, etc.)\n",
    "\n",
    "    Args:\n",
    "    data (List[List[Tuple[str, str]]]): A list of sentences, where each sentence is represented as a list of (word, POS tag) tuples.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Dict[str, int], Dict[str, int]]: A tuple containing two dictionaries. The first dictionary maps words to their index in the vocabulary, and the second dictionary maps POS tags to their index in the vocabulary.\n",
    "    \"\"\"  \n",
    "    # *****START OF YOUR CODE*****\n",
    "    \n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the training and validation data files using the `parse_file` function\n",
    "training_data = parse_file(\"./dataset/train.conllu\")\n",
    "validation_data = parse_file(\"./dataset/val.conllu\")\n",
    "\n",
    "# Build the vocabulary for the training data using the `build_vocab` function\n",
    "# The `build_vocab` function returns two dictionaries:\n",
    "#   - `word_to_idx`: maps words to their index in the vocabulary\n",
    "#   - `pos_to_idx`: maps POS tags to their index in the vocabulary\n",
    "word_to_idx, pos_to_idx = build_vocab(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions to convert between indices and human-readable format. You don't need to do anything here.\n",
    "#Just reading and making sure you understand what's going on is enough.\n",
    "\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "idx_to_pos = {idx: pos for pos, idx in pos_to_idx.items()}\n",
    "\n",
    "def convert_idx_to_words(indices: torch.tensor) -> str:\n",
    "    \"\"\"Converts a sequence of word indices to a human-readable format.\n",
    "    \n",
    "    Args:\n",
    "        indices (torch.tensor): A sequence of word indices.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string representation of the sequence of words.\n",
    "    \"\"\"\n",
    "    return \" \".join([idx_to_word[idx.item()] for idx in indices])\n",
    "\n",
    "def convert_idx_to_pos(indices: torch.tensor) -> str:\n",
    "    \"\"\"Converts a sequence of POS tag indices to a human-readable format.\n",
    "    \n",
    "    Args:\n",
    "        indices (torch.tensor): A sequence of POS tag indices.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string representation of the sequence of POS tags.\n",
    "    \"\"\"\n",
    "    return \" \".join([idx_to_pos[idx.item()] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function used for minibatching. You don't need to do anything here. Just reading and making sure you understand what's going on is enough.\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    This function collates a batch of sentences into a padded tensor that can be processed by the model.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    batch: a list of tuples where each tuple contains a sentence and its corresponding POS tags.\n",
    "    Returns:\n",
    "\n",
    "    A tuple of two padded tensors: one containing the text data and the other containing the POS tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_list, text_list = [], []\n",
    "    for (line, label) in batch:\n",
    "        text_list.append(line)\n",
    "        tag_list.append(label)\n",
    "        \n",
    "    return (\n",
    "        pad_sequence(text_list, padding_value=word_to_idx['<PAD>']),\n",
    "        pad_sequence(tag_list, padding_value=pos_to_idx['<PAD>'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class representing a Part-Of-Speech (POS) tagging dataset, which inherits from PyTorch's Dataset class.\n",
    "    You need to four methods for this class:\n",
    "    - __init__: Initializes the dataset object.\n",
    "    - __len__: Returns the number of sentences in the dataset.\n",
    "    - __getitem__: Returns the i-th sentence in the dataset.\n",
    "    - vocab_lookup: Converts a sentence represented as a list of word/POS-tag pairs (tuples) to a pair of PyTorch tensors \n",
    "                    representing the corresponding sequences of word and POS tag indices. Out of vocabulary words are\n",
    "                    represented by the index of the \"<UNK>\" token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: List[List[Tuple[str, str]]], word_to_idx: Dict, pos_to_idx: Dict):\n",
    "        \"\"\"\n",
    "        Initializes a new POSDataset object.\n",
    "        Args:\n",
    "        - data: A list of sentences, where each sentence is a list of word/POS-tag pairs (tuples).\n",
    "        - word_to_idx: A dictionary mapping words to their corresponding indices.\n",
    "        - pos_to_idx: A dictionary mapping POS tags to their corresponding indices.\n",
    "        \"\"\"\n",
    "        # *****START OF YOUR CODE*****\n",
    "        \n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE*****\n",
    "\n",
    "    def vocab_lookup(self, sentence: List[Tuple[str, str]]) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        Converts a sentence represented as a list of word/POS-tag pairs (tuples) to a pair of PyTorch tensors\n",
    "        representing the corresponding sequences of word and POS tag indices. Out of vocabulary words are\n",
    "        represented by the index of the \"<UNK>\" token.\n",
    "\n",
    "        Args:\n",
    "        - sentence: A list of word/POS-tag pairs (tuples) representing a single sentence.\n",
    "\n",
    "        Returns:\n",
    "        A tuple containing two PyTorch tensors, the first representing the sequence of word indices in the sentence,\n",
    "        and the second representing the sequence of POS tag indices in the sentence.\n",
    "        \"\"\"\n",
    "        # *****START OF YOUR CODE*****\n",
    "        \n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE*****\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of sentences in the dataset.\n",
    "        \"\"\"\n",
    "        # *****START OF YOUR CODE*****\n",
    "        \n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE*****\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        Returns a single sentence from the dataset as a pair of PyTorch tensors representing the corresponding\n",
    "        sequences of word and POS tag indices.\n",
    "\n",
    "        Args:\n",
    "        - idx: The index of the sentence to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        A tuple containing two PyTorch tensors, the first representing the sequence of word indices in the sentence,\n",
    "        and the second representing the sequence of POS tag indices in the sentence.\n",
    "        \"\"\"\n",
    "        # *****START OF YOUR CODE*****\n",
    "        \n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is preparing the training and validation datasets by creating POSDataset objects \n",
    "# using training_data and validation_data. The word_to_idx and pos_to_idx dictionaries created in build_vocab \n",
    "# are passed to POSDataset so that each sentence in the datasets can be converted to a tensor of word and POS tag indices.\n",
    "# Then, DataLoader objects are created for both the training and validation datasets, with BATCH_SIZE batches per iteration. \n",
    "# shuffle=True is used to shuffle the order of samples in each batch, which helps to prevent the model from overfitting to the order of the data. \n",
    "# collate_batch is used as the function to merge samples into batches, as it pads sequences to the same length and \n",
    "# returns two tensors, one for the word indices and one for the POS tag indices.\n",
    "# This code block is essential to prepare the data for training the model. \n",
    "# The training and validation dataloaders can be used in the training loop to iterate over the dataset in batches.\n",
    "\n",
    "training_dataset = None \n",
    "validation_dataset = None \n",
    "\n",
    "training_dataloader = None\n",
    "validation_dataloader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is always usefull to see dataset statistics to get a better understanding of the data.\n",
    "print(f\"Unique tokens in word vocabulary: {len(word_to_idx)}\")\n",
    "print(f\"Unique tokens in tag vocabulary: {len(pos_to_idx)}\")\n",
    "print()\n",
    "print(f\"Number of training examples: {len(training_dataset)}\")\n",
    "print(f\"Number of validation examples: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a random sample from the training dataset to see if the data is correctly loaded.\n",
    "print(\"Sample from the dataset:\", training_dataset[4])\n",
    "print()\n",
    "print(\"Human-readable version:\", convert_idx_to_words(training_dataset[4][0]), convert_idx_to_pos(training_dataset[4][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Implementation and Training\n",
    "\n",
    "In this part of the assignment, the focus is on Model Implementation and Training. This section involves the following steps:\n",
    "\n",
    "Implementing the BiLSTM POS Tagger model: In this step, you will create a class called BiLSTMPOSTagger that inherits from nn.Module. This class will be used for implementing the BiLSTM (bidirectional Long Short-Term Memory) network for POS tagging. The BiLSTM model will consist of an Embedding layer, an LSTM layer, a Dropout layer, and a Linear layer to make predictions.\n",
    "\n",
    "Defining training and evaluation functions: After implementing the model, you will need to define two essential functions for training and evaluation. The train_for_single_epoch function will be responsible for training the model for one epoch on the datasets. The evaluate function will be used for evaluating the model's performance on a given dataset. Both the functions will receive important arguments such as the model, dataset iterator, optimizer, and criterion (loss function).\n",
    "\n",
    "Running the training loop and observing the loss and accuracy: In this step, you will carry out the actual training. First, you will initialize the model, optimizer, and criterion. Then, you will run the training loop for a certain number of epochs (e.g., 10). In each epoch, you will train the model for one epoch using the train_for_single_epoch function and evaluate its performance on the validation dataset using the evaluate function. Finally, you will print and store the training loss and evaluation accuracy for each epoch.\n",
    "\n",
    "By the end of Part II, you will have a model that has been trained on the POS tagging dataset, and you can observe how the training process affects the loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        \"\"\"\n",
    "        BiLSTM model for POS tagging.\n",
    "        Check this link for more details: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of unique words in the vocabulary.\n",
    "            embedding_dim (int): Dimension of the word embeddings.\n",
    "            hidden_dim (int): Dimension of the LSTM hidden states.\n",
    "            output_dim (int): Number of unique POS tags.\n",
    "            n_layers (int): Number of LSTM layers.\n",
    "            bidirectional (bool): Whether to use a bidirectional LSTM.\n",
    "            dropout (float): Probability of dropout, if any.\n",
    "            pad_idx (int): Index of the <PAD> token in the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # *****START OF YOUR CODE*****\n",
    "        \n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE*****\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Perform forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            text (Tensor): Input text of shape [sent len, batch size].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predictions of shape [sent len, batch size, output dim].\n",
    "        \"\"\"\n",
    "        # *****START OF YOUR CODE*****\n",
    "        \n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inline question 3: How do you compare the advantages and disadvantages of using a bidirectional LSTM versus a unidirectional LSTM? #####\n",
    "<font color='red'>Your answer:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the output of the model for a random sample from the training dataset.\n",
    "# It is wrapped in torch.no_grad() because we are not training the model.\n",
    "with torch.no_grad():\n",
    "    inputs = training_dataset[0][0]\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_single_epoch(model, iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch on the given iterator with the specified optimizer and criterion.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to train.\n",
    "        iterator: The iterator over the training dataset.\n",
    "        optimizer: The optimizer to use for gradient descent.\n",
    "        criterion: The loss function to use.\n",
    "        tag_pad_idx: The index of the padding token in the tag vocabulary.\n",
    "        tag_unk_idx: The index of the unknown token in the tag vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        The average loss and accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE*****\n",
    "    \n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    \"\"\"\n",
    "    Returns the categorical accuracy between predictions and the ground truth, ignoring pad tokens.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE*****\n",
    "    \n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, tag_pad_idx):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a BiLSTMPOSTagger model on a given dataset iterator. Use the categorical_accuracy function\n",
    "    you implemented above to calculate the accuracy on a batch of predictions.\n",
    "\n",
    "    Args:\n",
    "    - model: a BiLSTMPOSTagger object.\n",
    "    - iterator: a DataLoader object containing (text, tags) tuples.\n",
    "    - tag_pad_idx: an integer representing the index of the padding token in the tag vocabulary.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the categorical accuracy of the model on the given dataset iterator.\n",
    "\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE*****\n",
    "    \n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the models accuracy without training\n",
    "accuracy = evaluate(model, validation_dataloader, tag_pad_idx=pos_to_idx['<PAD>'])\n",
    "print(f'Accuracy before training: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of random predictions\n",
    "epoch_correct = epoch_n_label = random_accuracy = most_frequent_accuracy = 0\n",
    "\n",
    "# *****START OF YOUR CODE*****\n",
    "\n",
    "pass\n",
    "\n",
    "# *****END OF YOUR CODE*****\n",
    "\n",
    "print(f'Accuracy of random predictions: {random_accuracy*100:.2f}%')\n",
    "\n",
    "# Calculate the accuracy of predicting the most frequent class\n",
    "\n",
    "# Get the most frequent class\n",
    "\n",
    "# *****START OF YOUR CODE*****\n",
    "\n",
    "pass\n",
    "\n",
    "# *****END OF YOUR CODE*****\n",
    "print(f'Accuracy of predicting the most frequent class: {most_frequent_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "\n",
    "NUM_OF_EPOCHS = None\n",
    "LEARNING_RATE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pos_to_idx['<PAD>']) #Modify this part if you are using a different padding token.\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "for x in range(NUM_OF_EPOCHS):\n",
    "    # Call the train_for_single_epoch function and store the result in the training_loss variable.\n",
    "    # Call the evaluate function and store the result in the validation_accuracy variable.\n",
    "    # Print out the current epoch number, training loss, and validation accuracy using the print function and formatted string syntax. \n",
    "    # Append the training_loss and validation_accuracy values to their respective history lists (loss_history and accuracy_history).\n",
    "    # *****START OF YOUR CODE*****\n",
    "    \n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot the training metrics, use the matplotlib library.\n",
    "\n",
    "# *****START OF YOUR CODE*****\n",
    "\n",
    "pass\n",
    "\n",
    "# *****END OF YOUR CODE*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models accuracy after training\n",
    "accuracy = evaluate(model, validation_dataloader, tag_pad_idx=pos_to_idx['<PAD>'])\n",
    "print(f'Accuracy after training: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'pos_tagger_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inline question 4: What modifications you need to make to convert this model to a character-level BiLSTM POS tagger?\n",
    "<font color='red'>Your answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Initializing BiLSTM with fastText Embeddings\n",
    "\n",
    "In Part III of this project, you will be combining the power of fastText embeddings with the sequence modeling capability of the BiLSTM model. You will load the pretrained fastText model for Turkish, which was trained by you on a large corpus of Turkish text. The pre-trained model can generate word embeddings for any Turkish word, including words that are not in the training data for our specific task. This is an effective approach to handling the out-of-vocabulary (OOV) words problem that can occur in natural language processing tasks.\n",
    "\n",
    "After initializing the models, we will train the model on our dataset and evaluate its performance on a held-out development set (same dataset as above). By combining these two powerful models, we hope to achieve better accuracy and robustness in our POS tagging task.\n",
    "\n",
    "* Optional: Try the embeddings of both skipgram and cbow approach for your evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = None\n",
    "# Load the FastText pre-trained embeddings and set them as the model's embedding layer\n",
    "# pretrained_embeddings = None\n",
    "# model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "\n",
    "NUM_OF_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pos_to_idx['<PAD>'])\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves\n",
    "# *****START OF YOUR CODE*****\n",
    "\n",
    "pass\n",
    "\n",
    "# *****END OF YOUR CODE*****"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "905F15FA642C45E5AD49BD1B07803981",
   "lastKernelId": "3cb236bf-6258-421e-a93f-3bbf43d85854"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
